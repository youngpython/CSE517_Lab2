function BDT=boosttree(x,y,nt,maxdepth)
% function BDT=boosttree(x,y,nt,maxdepth)
%
% Learns a boosted decision tree on data x with labels y.
% It performs at most nt boosting iterations. Each decision tree has maximum depth "maxdepth".
%
% INPUT: 
% x  | input vectors dxn
% y  | input labels 1xn
% nt | number of trees (default = 100)
% maxdepth | depth of each tree (default = 3)
%
% OUTPUT:
% BDT | Boosted DTree
%
if ~exist('nt', 'var')==1 || isempty(nt)
   nt = 100; 
end
if ~exist('maxdepth', 'var')==1 || isempty(maxdepth)
   maxdepth = 3; 
end
[~,n] = size(x);
BDT = zeros(6,n);
weights = ones(1,n)./n;
for m = 1:nt
   T = id3tree(x,y,maxdepth,weights); %FIXME Prune?
   h_x = evaltree(T,x);
   indicator = y ~= h_x; %FIXME Are the right ones 0 or -1
   err = sum(indicator.*weights);
   if err > .5
       break;
   end
   alpha = (1/2)*log((1-err)/err);
   %BDT = BDT + alpha*T; %FIXME In the pseudocode, but Dr. Weinberger says
   %we don't have to
   
   %Convert predictions and y vectors to 1's and -1's (rather than 1's
   %and 2's or whatever they may be)
   pos = max(y);
   neg = min(y);
   h_x(h_x == neg) = -1;
   h_x(h_x == pos) = 1; %Only relevant when training label 
   y_dummy = y; 
   y_dummy(y_dummy == neg) = -1;
   y_dummy(y_dummy == pos) = 1;
   for i = 1:n
      weights(i) = weights(i)*exp(-1*alpha*h_x(i)*y_dummy(i)); 
   end
   weights = weights/norm(weights);
end
BDT = id3tree(x,y,maxdepth,weights);
